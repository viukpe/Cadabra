# Order History Functionality

![ScreenShot](/assets/images/01.png)

This implementation is a data pipeline to store and retrieve order history data using AWS services - EC2, Kinesis, Lambda, and DynamoDB. 

**Use Case:** Placing an order on the Cadabra app and instantly viewing the order in the app's "Past Orders" section, along with a comprehensive history from previously made orders.

## Steps in the Data Pipeline

### 1. Simulate mock data on EC2

Amazon EC2 (Elastic Compute Cloud) instance was used to simulate live data being generated. The `OnlineRetail.csv` and `LogGenerator.py` were loaded on the instance, and the Python file was used to generate order records on command.


### 2. Data Ingestion with Kinesis

Amazon Kinesis was used to ingest and process the real-time streaming data generated by the EC2 instance. A kinesis agent has to be configured and set in the EC2 instance. The job of the agent, in general, is to monitor your data producer for new input and send it to a pre-configured data stream; like in our case, we'll use it to monitor the logs generated from the Python file and send the data to the set Kinesis Data Streams for ingestion.

### 3. Data Transformation with Lambda

AWS Lambda functions were employed to perform data transformation on the streaming data received from Kinesis and served as a handler to put the data in a pre-configured DynamoDB table. 

### 4. Data Storage in DynamoDB

Amazon DynamoDB is a NoSQL database service, and it was used to store the processed order history data. DynamoDB offers seamless scalability and low-latency performance, making it suitable for fast and reliable data retrieval applications.

![ScreenShot](/assets/images/DynamoDB.png)

## Further steps
- The most common way to retrieve data in the AWS cloud would be with API Gateway, so configuring an API to read data from the dynamoDB table would be all that's left. Now, for example, when you visit the "past orders" url page on the Cadabra app, you'll be able to view data stored in the DynamoDB table.



## Prerequisites

Before setting up and running this data pipeline, ensure that you have:

- AWS account credentials with appropriate permissions to create and manage EC2 instances, Kinesis data streams, Lambda functions, and DynamoDB tables.

- Familiarity with AWS management console for deployment and configuration.

